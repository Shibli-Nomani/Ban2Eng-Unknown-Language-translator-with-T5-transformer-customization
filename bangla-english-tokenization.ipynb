{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12847512,"sourceType":"datasetVersion","datasetId":8125875}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìö Import Libraries","metadata":{}},{"cell_type":"code","source":"# -------------------------------\n# Import Required Libraries\n# -------------------------------\n# - pandas: data handling\n# - sentencepiece: tokenizer training\n# - zipfile, os: file handling\n# - collections.Counter: word frequency analysis\n# - For displaying\n\nimport pandas as pd\nimport sentencepiece as spm\nimport zipfile\nimport os\nfrom collections import Counter\nfrom IPython.display import display, Markdown\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:19:06.180137Z","iopub.execute_input":"2025-08-24T19:19:06.181161Z","iopub.status.idle":"2025-08-24T19:19:08.271073Z","shell.execute_reply.started":"2025-08-24T19:19:06.181127Z","shell.execute_reply":"2025-08-24T19:19:08.269890Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# üóÇÔ∏è Load and View Datasets","metadata":{}},{"cell_type":"markdown","source":"## üîç Insight of Dataset\n\n‚ú® Dataset has been prepared with the help of ChatGPT, Gemeni, and Grok for Bangla-English Text Generation and Translation","metadata":{}},{"cell_type":"code","source":"#=================================\n# Train Dataset\n#=================================\ntraindataset = pd.read_csv(\"/kaggle/input/bangla-english-custom-dataset/final-datasets/train-data.csv\")\nprint(f\"Shape of traindataset: {traindataset.shape}\")\nprint(\"============================About Training Dataset===========================\")\ntraindataset.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:19:08.272873Z","iopub.execute_input":"2025-08-24T19:19:08.273368Z","iopub.status.idle":"2025-08-24T19:19:08.390345Z","shell.execute_reply.started":"2025-08-24T19:19:08.273340Z","shell.execute_reply":"2025-08-24T19:19:08.389489Z"}},"outputs":[{"name":"stdout","text":"Shape of traindataset: (4008, 2)\n============================About Training Dataset===========================\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                Text  \\\n0  Apni kemon achhen ajke? Ami chinta korchilam a...   \n1  Apni recently kemon feel korchen? Ami chinta k...   \n2  Apni ki ajke kichu emotional feel korchen? Ami...   \n3  Apni ki recently kono boro decision niyechhen?...   \n4  Apni ki recently kichu kichu stress feel korch...   \n\n                                              Target  \n0  How are you today? I was thinking whether you ...  \n1  How have you been feeling lately? I was thinki...  \n2  Are you feeling emotional today? I was thinkin...  \n3  Have you recently made any big decision? I was...  \n4  Have you recently felt some stress? I was thin...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Apni kemon achhen ajke? Ami chinta korchilam a...</td>\n      <td>How are you today? I was thinking whether you ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apni recently kemon feel korchen? Ami chinta k...</td>\n      <td>How have you been feeling lately? I was thinki...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Apni ki ajke kichu emotional feel korchen? Ami...</td>\n      <td>Are you feeling emotional today? I was thinkin...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Apni ki recently kono boro decision niyechhen?...</td>\n      <td>Have you recently made any big decision? I was...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Apni ki recently kichu kichu stress feel korch...</td>\n      <td>Have you recently felt some stress? I was thin...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## About Dataset","metadata":{}},{"cell_type":"code","source":"print(f\"Bangla English mixed written in English: \\n {traindataset.iloc[300]}\\n\")\n\n\n\nprint(f\"Bangla English mixed Bangla words are written Bangla and English words are written in English: \\n {traindataset.iloc[300]}\\n\")\n\nprint(f\"Purely written in Bangla:\\n {traindataset.iloc[1600]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:19:11.077567Z","iopub.execute_input":"2025-08-24T19:19:11.078455Z","iopub.status.idle":"2025-08-24T19:19:11.086822Z","shell.execute_reply.started":"2025-08-24T19:19:11.078425Z","shell.execute_reply":"2025-08-24T19:19:11.085382Z"}},"outputs":[{"name":"stdout","text":"Bangla English mixed written in English: \n Text      Apni ki ajke parcel pick-up request korben? Am...\nTarget    Will you request a parcel pick-up today? I was...\nName: 300, dtype: object\n\nBangla English mixed Bangla words are written Bangla and English words are written in English: \n Text      Apni ki ajke parcel pick-up request korben? Am...\nTarget    Will you request a parcel pick-up today? I was...\nName: 300, dtype: object\n\nPurely written in Bangla:\n Text      ‡¶¶‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶ú‡¶ï‡ßá‡¶∞ ‡¶°‡ßá‡¶°‡¶≤‡¶æ‡¶á‡¶® ‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡ßÅ‡¶®‡•§ ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶∏‡¶Æ‡¶Ø‡¶º‡¶Æ‡¶§‡ßã...\nTarget    Please remember today‚Äôs deadline. We want to f...\nName: 1600, dtype: object\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#=================================\n# Valid Dataset\n#=================================\n\nvaldataset = pd.read_csv(\"/kaggle/input/bangla-english-custom-dataset/final-datasets/val-data.csv\")\nprint(f\"Shape of valdataset: {valdataset.shape}\")\nprint(\"============================About Validation Dataset===========================\")\nvaldataset.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:19:18.462943Z","iopub.execute_input":"2025-08-24T19:19:18.463271Z","iopub.status.idle":"2025-08-24T19:19:18.503457Z","shell.execute_reply.started":"2025-08-24T19:19:18.463240Z","shell.execute_reply":"2025-08-24T19:19:18.502493Z"}},"outputs":[{"name":"stdout","text":"Shape of valdataset: (945, 2)\n============================About Validation Dataset===========================\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                Text  \\\n0  Apnar kotha shune amar khub shanti lage. Kichu...   \n1  Apni ajker jonno ki plan korechhen? Jodi time ...   \n2  Ajke office e khub pressure chhilo. Apni ki kh...   \n3  Ajke onek rush chhilo rastay. Apni ki safe e b...   \n4  Apnar shathe kotha bole amar mon ektu halka ho...   \n\n                                              Target  \n0  Listening to you gives me peace. I was a bit s...  \n1  What plans do you have for today? If you have ...  \n2  There was a lot of pressure at the office toda...  \n3  The roads were very rushed today. Did you retu...  \n4  Talking with you makes my mind lighter. I try ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Apnar kotha shune amar khub shanti lage. Kichu...</td>\n      <td>Listening to you gives me peace. I was a bit s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apni ajker jonno ki plan korechhen? Jodi time ...</td>\n      <td>What plans do you have for today? If you have ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ajke office e khub pressure chhilo. Apni ki kh...</td>\n      <td>There was a lot of pressure at the office toda...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ajke onek rush chhilo rastay. Apni ki safe e b...</td>\n      <td>The roads were very rushed today. Did you retu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Apnar shathe kotha bole amar mon ektu halka ho...</td>\n      <td>Talking with you makes my mind lighter. I try ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"#=================================\n# Unseen Dataset  / Test Dataset\n#=================================\n\nunseendataset = pd.read_csv(\"/kaggle/input/bangla-english-custom-dataset/final-datasets/test-data.csv\")\nprint(f\"Shape of unseendataset: {unseendataset.shape}\")\nprint(\"============================About Unseen Dataset===========================\")\nunseendataset.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:19:24.428434Z","iopub.execute_input":"2025-08-24T19:19:24.428776Z","iopub.status.idle":"2025-08-24T19:19:24.461217Z","shell.execute_reply.started":"2025-08-24T19:19:24.428747Z","shell.execute_reply":"2025-08-24T19:19:24.460186Z"}},"outputs":[{"name":"stdout","text":"Shape of unseendataset: (167, 2)\n============================About Unseen Dataset===========================\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                Text  \\\n0  ‡¶∂‡ßÅ‡¶≠ ‡¶®‡¶¨‡¶¨‡¶∞‡ßç‡¶∑! ‡¶™‡¶π‡ßá‡¶≤‡¶æ ‡¶¨‡ßà‡¶∂‡¶æ‡¶ñ‡ßá ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶®‡¶§‡ßÅ‡¶® ‡¶ú‡¶æ‡¶Æ‡¶æ ‡¶™‡¶∞‡ßá...   \n1  ‡¶ú‡¶®‡ßç‡¶Æ‡¶¶‡¶ø‡¶®‡ßá‡¶∞ ‡¶∂‡ßÅ‡¶≠‡ßá‡¶ö‡ßç‡¶õ‡¶æ! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶ú‡¶®‡ßç‡¶Æ‡¶¶‡¶ø‡¶®‡ßá ‡¶ï‡ßá‡¶ï ‡¶ï‡¶æ‡¶ü‡¶¨‡ßá...   \n2  ‡¶∂‡ßÅ‡¶≠ ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞ ‡¶ú‡¶Ø‡¶º‡¶®‡ßç‡¶§‡ßÄ! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶•‡ßá‡¶∞ ‡¶ó‡¶æ...   \n3  ‡¶™‡¶π‡ßá‡¶≤‡¶æ ‡¶´‡¶æ‡¶≤‡ßç‡¶ó‡ßÅ‡¶®‡ßá‡¶∞ ‡¶∂‡ßÅ‡¶≠‡ßá‡¶ö‡ßç‡¶õ‡¶æ! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶´‡ßÅ‡¶≤‡ßá‡¶∞ ‡¶ú‡¶æ‡¶Æ‡¶æ ‡¶™...   \n4  ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶æ ‡¶¶‡¶ø‡¶¨‡¶∏‡ßá‡¶∞ ‡¶∂‡ßÅ‡¶≠‡¶ï‡¶æ‡¶Æ‡¶®‡¶æ! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶ú‡¶®‡ßá‡¶∞ ‡¶∏...   \n\n                                              Target  \n0  Happy New Year! Rocking a new outfit for Pohel...  \n1  Happy Birthday! Cutting a cake for your birthd...  \n2  Happy Rabindra Jayanti! Singing Tagore‚Äôs songs...  \n3  Happy Pohela Falgun! Wearing a floral outfit? ...  \n4  Happy Valentine‚Äôs Day! Planning a dinner with ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>‡¶∂‡ßÅ‡¶≠ ‡¶®‡¶¨‡¶¨‡¶∞‡ßç‡¶∑! ‡¶™‡¶π‡ßá‡¶≤‡¶æ ‡¶¨‡ßà‡¶∂‡¶æ‡¶ñ‡ßá ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶®‡¶§‡ßÅ‡¶® ‡¶ú‡¶æ‡¶Æ‡¶æ ‡¶™‡¶∞‡ßá...</td>\n      <td>Happy New Year! Rocking a new outfit for Pohel...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>‡¶ú‡¶®‡ßç‡¶Æ‡¶¶‡¶ø‡¶®‡ßá‡¶∞ ‡¶∂‡ßÅ‡¶≠‡ßá‡¶ö‡ßç‡¶õ‡¶æ! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶ú‡¶®‡ßç‡¶Æ‡¶¶‡¶ø‡¶®‡ßá ‡¶ï‡ßá‡¶ï ‡¶ï‡¶æ‡¶ü‡¶¨‡ßá...</td>\n      <td>Happy Birthday! Cutting a cake for your birthd...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>‡¶∂‡ßÅ‡¶≠ ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞ ‡¶ú‡¶Ø‡¶º‡¶®‡ßç‡¶§‡ßÄ! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶•‡ßá‡¶∞ ‡¶ó‡¶æ...</td>\n      <td>Happy Rabindra Jayanti! Singing Tagore‚Äôs songs...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>‡¶™‡¶π‡ßá‡¶≤‡¶æ ‡¶´‡¶æ‡¶≤‡ßç‡¶ó‡ßÅ‡¶®‡ßá‡¶∞ ‡¶∂‡ßÅ‡¶≠‡ßá‡¶ö‡ßç‡¶õ‡¶æ! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶´‡ßÅ‡¶≤‡ßá‡¶∞ ‡¶ú‡¶æ‡¶Æ‡¶æ ‡¶™...</td>\n      <td>Happy Pohela Falgun! Wearing a floral outfit? ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶æ ‡¶¶‡¶ø‡¶¨‡¶∏‡ßá‡¶∞ ‡¶∂‡ßÅ‡¶≠‡¶ï‡¶æ‡¶Æ‡¶®‡¶æ! ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶™‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶ú‡¶®‡ßá‡¶∞ ‡¶∏...</td>\n      <td>Happy Valentine‚Äôs Day! Planning a dinner with ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# üîÑ Merging Train and Val Dataset","metadata":{}},{"cell_type":"code","source":"#====================================================\n# Joining Train and Valid Data for Vocab Extraction\n#====================================================\n\n# Merge train & validation datasets by row\nfinaldataset = pd.concat([traindataset, valdataset], axis=0, ignore_index=True)\n\n# Use full dataset since total rows = 4953\nsubset = finaldataset.sample(n=finaldataset.shape[0], random_state=42)\n\nprint(f\"‚úÖ Shape of Dataset after merging: {finaldataset.shape}\")\nprint(\"============== Final Dataset After Merging ==================\")\nfinaldataset.head(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:19:31.287888Z","iopub.execute_input":"2025-08-24T19:19:31.288245Z","iopub.status.idle":"2025-08-24T19:19:31.303368Z","shell.execute_reply.started":"2025-08-24T19:19:31.288207Z","shell.execute_reply":"2025-08-24T19:19:31.302420Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Shape of Dataset after merging: (4953, 2)\n============== Final Dataset After Merging ==================\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                Text  \\\n0  Apni kemon achhen ajke? Ami chinta korchilam a...   \n1  Apni recently kemon feel korchen? Ami chinta k...   \n2  Apni ki ajke kichu emotional feel korchen? Ami...   \n3  Apni ki recently kono boro decision niyechhen?...   \n4  Apni ki recently kichu kichu stress feel korch...   \n\n                                              Target  \n0  How are you today? I was thinking whether you ...  \n1  How have you been feeling lately? I was thinki...  \n2  Are you feeling emotional today? I was thinkin...  \n3  Have you recently made any big decision? I was...  \n4  Have you recently felt some stress? I was thin...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Apni kemon achhen ajke? Ami chinta korchilam a...</td>\n      <td>How are you today? I was thinking whether you ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apni recently kemon feel korchen? Ami chinta k...</td>\n      <td>How have you been feeling lately? I was thinki...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Apni ki ajke kichu emotional feel korchen? Ami...</td>\n      <td>Are you feeling emotional today? I was thinkin...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Apni ki recently kono boro decision niyechhen?...</td>\n      <td>Have you recently made any big decision? I was...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Apni ki recently kichu kichu stress feel korch...</td>\n      <td>Have you recently felt some stress? I was thin...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# üìù Generate Corpus","metadata":{}},{"cell_type":"code","source":"# Load CSV\n# Merge both columns into one text corpus\ncorpus = pd.concat([subset[\"Text\"], subset[\"Target\"]], axis=0)\n\n# Save to plain text file for tokenizer training\nwith open(\"/kaggle/working/corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n    for line in corpus:\n        f.write(str(line).strip() + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:19:37.401936Z","iopub.execute_input":"2025-08-24T19:19:37.402299Z","iopub.status.idle":"2025-08-24T19:19:37.424173Z","shell.execute_reply.started":"2025-08-24T19:19:37.402271Z","shell.execute_reply":"2025-08-24T19:19:37.422694Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# -------------------------------\n# Analyze Corpus: Word Count & Frequency\n# -------------------------------\n# - Reads the merged corpus.txt\n# - Counts total words and unique words\n# - Displays top 30 most frequent words\n\n# -------------------------------\n# Read the corpus\n# -------------------------------\nwith open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n    text = f.read()\n\n# -------------------------------\n# Split text into words (basic whitespace tokenization)\n# -------------------------------\nwords = text.split()\n\n# -------------------------------\n# Count total and unique words\n# -------------------------------\ntotal_words = len(words)\nunique_words = len(set(words))\n\nprint(f\"Total words in corpus: {total_words}\")\nprint(f\"Unique words in corpus: {unique_words}\")\n\n# -------------------------------\n# Compute word frequencies\n# -------------------------------\nword_freq = Counter(words).most_common(30)\n\n# -------------------------------\n# Display top 30 frequent words\n# -------------------------------\nprint(\"\\nTop 30 words:\\n\", word_freq)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:19:42.019527Z","iopub.execute_input":"2025-08-24T19:19:42.019860Z","iopub.status.idle":"2025-08-24T19:19:42.129348Z","shell.execute_reply.started":"2025-08-24T19:19:42.019836Z","shell.execute_reply":"2025-08-24T19:19:42.128403Z"}},"outputs":[{"name":"stdout","text":"Total words in corpus: 251684\nUnique words in corpus: 17675\n\nTop 30 words:\n [('I', 6125), ('the', 5804), ('a', 4241), ('to', 2615), ('‡¶Ü‡¶Æ‡¶ø', 2219), ('you', 2162), ('is', 2009), ('was', 1980), ('and', 1859), ('very', 1661), ('‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§', 1446), ('The', 1337), ('properly', 1335), ('with', 1238), ('or', 1234), ('new', 1180), ('Apni', 1164), ('‡¶è‡¶¨‡¶Ç', 1149), ('for', 1111), ('ki', 1097), ('my', 1031), ('will', 943), ('Friends', 941), ('‡¶ï‡¶ø', 913), ('in', 908), ('now!', 891), ('‡¶è‡¶ñ‡¶®', 835), ('‡¶ñ‡ßÅ‡¶¨', 829), ('of', 829), ('Ami', 822)]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# ‚è≥ Sentence Tokenization For Bangla & English Mixed Corpus","metadata":{}},{"cell_type":"code","source":"# -------------------------------\n# Train a SentencePiece tokenizer\n# -------------------------------\n# - corpus.txt: contains your source language + English text\n# - custom_spm.model & custom_spm.vocab: output files\n# - vocab_size: number of tokens/subwords in the vocabulary\n# - character_coverage: how much of your text characters to cover (0.9995 ~ 99.95%)\n# - model_type: 'bpe' (Byte-Pair Encoding) for mixed-language & code\n# - shuffle_input_sentence: whether to shuffle sentences during training\n# - input_sentence_size: maximum number of sentences to use for training\n\n# Path to your merged corpus\ncorpus_path = \"/kaggle/working/corpus.txt\"\n\n# output files: custom_spm.model + custom_spm.vocab\n# matches unique words count\n# cover 99.95% of characters\n# subword algorithm\n# shuffle sentences\n# upper limit for training sentences\n# use 4 CPU threads (adjust if needed)\n\n# Train the SentencePiece tokenizer\nspm.SentencePieceTrainer.train(\n    input=corpus_path,\n    model_prefix=\"custom_spm\",\n    vocab_size=9338,                 # must not exceed runtime limit\n    character_coverage=0.9995,\n    model_type=\"unigram\",            # better for mixed-language\n    byte_fallback=True,              # fallback for rare characters\n    split_by_whitespace=True,        # preserve full words\n    shuffle_input_sentence=True,\n    input_sentence_size=100000,      \n    max_sentence_length=2048,        \n    num_threads=4\n)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:46:30.161251Z","iopub.execute_input":"2025-08-24T19:46:30.161652Z","iopub.status.idle":"2025-08-24T19:46:31.602108Z","shell.execute_reply.started":"2025-08-24T19:46:30.161627Z","shell.execute_reply":"2025-08-24T19:46:31.598878Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /kaggle/working/corpus.txt\n  input_format: \n  model_prefix: custom_spm\n  model_type: UNIGRAM\n  vocab_size: 9338\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 100000\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 2048\n  num_threads: 4\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 1\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ‚Åá \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/working/corpus.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 9906 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=1465605\ntrainer_interface.cc(550) LOG(INFO) Done: 99.958% characters are covered.\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=112\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=0.99958\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 9906 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=901009\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 27009 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 9906\ntrainer_interface.cc(609) LOG(INFO) Done! 17534\nunigram_model_trainer.cc(602) LOG(INFO) Using 17534 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11578 obj=12.1477 num_tokens=34655 num_tokens/piece=2.99318\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9066 obj=9.95599 num_tokens=34759 num_tokens/piece=3.834\ntrainer_interface.cc(687) LOG(INFO) Saving model: custom_spm.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: custom_spm.vocab\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"**‚ö†Ô∏è Note: **Sometimes the cell shows **`*`** for a long time ‚Äî go to Run ‚Üí Restart & Clear Cell Outputs and re-run.\nüìÑ After that, **`custom_spm.model`** and **`custom_spm.vocab`** will appear in your working directory.\n\n**Process will hardly take 5 - 10 min for Sentence Tokenization.**","metadata":{}},{"cell_type":"code","source":"print(\"‚úÖ SentencePiece tokenizer training complete!\")\nprint(\"Generated files: custom_spm.model, custom_spm.vocab\")\n\n# -------------------------------\n# Test the tokenizer\n# -------------------------------\nsp = spm.SentencePieceProcessor(model_file=\"custom_spm.model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:46:43.945084Z","iopub.execute_input":"2025-08-24T19:46:43.945689Z","iopub.status.idle":"2025-08-24T19:46:43.974646Z","shell.execute_reply.started":"2025-08-24T19:46:43.945639Z","shell.execute_reply":"2025-08-24T19:46:43.973418Z"}},"outputs":[{"name":"stdout","text":"‚úÖ SentencePiece tokenizer training complete!\nGenerated files: custom_spm.model, custom_spm.vocab\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# ‚úÖ Result","metadata":{}},{"cell_type":"code","source":"\n# Example test sentences (Bangla + English)\nsamples = [\n    \"‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶ú‡¶ï‡ßá school ‡¶è ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø\",\n    \"This dataset is really useful for NLP.\",\n    \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶¨‡¶Ç English ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶Ü‡¶õ‡ßá‡•§\",\n    \"Ami ajke apnar sathe meet korbo\"\n]\n\nfor s in samples:\n    ids = sp.encode(s, out_type=int)\n    tokens = sp.encode(s, out_type=str)\n    print(\"\\nInput:\", s)\n    print(\"Tokens:\", tokens)\n    print(\"IDs:\", ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:46:48.318227Z","iopub.execute_input":"2025-08-24T19:46:48.318576Z","iopub.status.idle":"2025-08-24T19:46:48.326284Z","shell.execute_reply.started":"2025-08-24T19:46:48.318556Z","shell.execute_reply":"2025-08-24T19:46:48.325126Z"}},"outputs":[{"name":"stdout","text":"\nInput: ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶ú‡¶ï‡ßá school ‡¶è ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø\nTokens: ['‚ñÅ‡¶Ü‡¶Æ‡¶ø', '‚ñÅ‡¶Ü‡¶ú‡¶ï‡ßá', '‚ñÅschool', '‚ñÅ‡¶è', '‚ñÅ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø']\nIDs: [274, 318, 1431, 295, 2488]\n\nInput: This dataset is really useful for NLP.\nTokens: ['‚ñÅ', 'This', '‚ñÅdata', 'set', '‚ñÅis', '‚ñÅreal', 'ly', '‚ñÅuseful', '‚ñÅfor', '‚ñÅN', 'L', 'P', '.']\nIDs: [262, 424, 1998, 6083, 278, 1024, 494, 2096, 297, 2555, 9330, 3269, 259]\n\nInput: ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶¨‡¶Ç English ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶Ü‡¶õ‡ßá‡•§\nTokens: ['‚ñÅ‡¶¨‡¶æ', '‡¶Ç', '‡¶≤‡¶æ', '‚ñÅ‡¶è‡¶¨', '‡¶Ç', '‚ñÅEng', 'lish', '‚ñÅ‡¶è‡¶ï', '‡¶∏‡¶æ‡¶•‡ßá', '‚ñÅ‡¶Ü‡¶õ‡ßá', '‡•§']\nIDs: [1169, 292, 2313, 294, 292, 6500, 4854, 393, 546, 497, 260]\n\nInput: Ami ajke apnar sathe meet korbo\nTokens: ['‚ñÅAm', 'i', '‚ñÅajke', '‚ñÅapna', 'r', '‚ñÅsath', 'e', '‚ñÅmeet', '‚ñÅkorbo']\nIDs: [307, 273, 469, 876, 341, 783, 279, 1439, 1011]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Example sentences\nsentences = [\n    \"‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶ú‡¶ï‡ßá school ‡¶è ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø\",\n    \"This dataset is really useful for NLP.\",\n    \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶¨‡¶Ç English ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶Ü‡¶õ‡ßá‡•§\"\n]\n\n# Prepare Markdown table\nmd_table = \"| Input | Tokens | IDs |\\n|-------|--------|----|\\n\"\n\nfor sentence in sentences:\n    tokens = sp.encode(sentence, out_type=str)\n    ids = sp.encode(sentence, out_type=int)\n    md_table += f\"| {sentence} | {tokens} | {ids} |\\n\"\n\n# Display as Markdown\ndisplay(Markdown(md_table))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:46:57.772674Z","iopub.execute_input":"2025-08-24T19:46:57.773054Z","iopub.status.idle":"2025-08-24T19:46:57.783251Z","shell.execute_reply.started":"2025-08-24T19:46:57.773030Z","shell.execute_reply":"2025-08-24T19:46:57.781777Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"| Input | Tokens | IDs |\n|-------|--------|----|\n| ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶ú‡¶ï‡ßá school ‡¶è ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø | ['‚ñÅ‡¶Ü‡¶Æ‡¶ø', '‚ñÅ‡¶Ü‡¶ú‡¶ï‡ßá', '‚ñÅschool', '‚ñÅ‡¶è', '‚ñÅ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø'] | [274, 318, 1431, 295, 2488] |\n| This dataset is really useful for NLP. | ['‚ñÅ', 'This', '‚ñÅdata', 'set', '‚ñÅis', '‚ñÅreal', 'ly', '‚ñÅuseful', '‚ñÅfor', '‚ñÅN', 'L', 'P', '.'] | [262, 424, 1998, 6083, 278, 1024, 494, 2096, 297, 2555, 9330, 3269, 259] |\n| ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶¨‡¶Ç English ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶Ü‡¶õ‡ßá‡•§ | ['‚ñÅ‡¶¨‡¶æ', '‡¶Ç', '‡¶≤‡¶æ', '‚ñÅ‡¶è‡¶¨', '‡¶Ç', '‚ñÅEng', 'lish', '‚ñÅ‡¶è‡¶ï', '‡¶∏‡¶æ‡¶•‡ßá', '‚ñÅ‡¶Ü‡¶õ‡ßá', '‡•§'] | [1169, 292, 2313, 294, 292, 6500, 4854, 393, 546, 497, 260] |\n"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"# üìÇ Zip the relavant file for download","metadata":{}},{"cell_type":"code","source":"# -------------------------------\n# Zip Corpus and Tokenizer Files\n# -------------------------------\n# - Include corpus.txt, custom_spm.model, custom_spm.vocab\n# - Save as tokenizer_corpus.zip\n\nfiles_to_zip = [\"corpus.txt\", \"custom_spm.model\", \"custom_spm.vocab\"]\nzip_filename = \"tokenizer_corpus.zip\"\n\nwith zipfile.ZipFile(zip_filename, \"w\") as zipf:\n    for file in files_to_zip:\n        zipf.write(file, os.path.basename(file))  # save without full path\n\nprint(f\"ZIP file created: {zip_filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T19:47:05.004943Z","iopub.execute_input":"2025-08-24T19:47:05.005441Z","iopub.status.idle":"2025-08-24T19:47:05.026791Z","shell.execute_reply.started":"2025-08-24T19:47:05.005417Z","shell.execute_reply":"2025-08-24T19:47:05.024759Z"}},"outputs":[{"name":"stdout","text":"ZIP file created: tokenizer_corpus.zip\n","output_type":"stream"}],"execution_count":26}]}