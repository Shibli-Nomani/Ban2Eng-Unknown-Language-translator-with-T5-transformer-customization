# Project Overview (Unknown Language Translation to English)

- 1ï¸âƒ£ New words Tokenization: https://www.kaggle.com/code/shiblinomani/bangla-english-tokenization
- 2ï¸âƒ£ Datasets
- 3ï¸âƒ£ Custom Model Training Code: https://www.kaggle.com/code/shiblinomani/unknown-language-translation-in-english
- 4ï¸âƒ£ Github/github:https://github.com/Shibli-Nomani/Ban2Eng-Unknown-Language-translator-with-T5-transformer-customization
- 5ï¸âƒ£ Workflow:

```mermaid
    flowchart TD
    A[ğŸ“ Text Generation <br> Grok, ChatGPT, Gemeni] --> B[âœ‚ï¸ Sentence Tokenization<br>of Mixed Bangla-English for .vocab]
    B --> C[ğŸ¤– Choose Model: T5 Transformer<br>Unknown of Bangla Language]
    C --> |Config, Adding Layers| D[ğŸ› ï¸ T5 Model Customization]
    D --> E[â• Add New Vocab into Model Vocabulary]
    E --> F[ğŸ§¹ Data Preprocessing & Tokenization]
    F --> G[ğŸ‹ï¸ Model Training]
    G --> H[ğŸ“Š Model Evaluation:<br>BLEU, Log Likelihood,<br>Perplexity, ROUGE]

    
    style A fill:#f7c6c7,stroke:#000,stroke-width:2px,color:#000
    style B fill:#fceabb,stroke:#000,stroke-width:2px,color:#000
    style C fill:#c6e0f5,stroke:#000,stroke-width:2px,color:#000
    style D fill:#d5f5e3,stroke:#000,stroke-width:2px,color:#000
    style E fill:#f9d5e5,stroke:#000,stroke-width:2px,color:#000
    style F fill:#ffe5b4,stroke:#000,stroke-width:2px,color:#000
    style G fill:#d1c4e9,stroke:#000,stroke-width:2px,color:#000
    style H fill:#a2d5f2,stroke:#000,stroke-width:2px,color:#000

```

### T5-Small (~60M)
```mermaid
graph LR
    A1["T5ForConditionalGeneration<br>~60M<br>parameters"] --> 
    B1["Shared Embedding<br>32128Ã—512<br>â‰ˆ 16.4M"] --> 
    C1["Encoder 6 layers<br>total â‰ˆ 19.2M"] --> 
    C1a["Each Encoder Block<br>â‰ˆ 3.2M<br>- Self-Attention â‰ˆ 1.05M<br>- Feed Forward â‰ˆ 2.1M"] --> 
    D1["Decoder 6 layers<br>total â‰ˆ 25.2M"] --> 
    D1a["Each Decoder Block<br>â‰ˆ 4.2M<br>- Self-Attention â‰ˆ 1.05M<br>- Cross-Attention â‰ˆ 1.05M<br>- Feed Forward â‰ˆ 2.1M"] --> 
    E1["LM Head<br>512â†’32128<br>â‰ˆ 16.4M"]

style A1 fill:#dce6f9,stroke:#333,stroke-width:2px,color:#000,font-size:20px
style B1 fill:#e8dcf9,stroke:#333,stroke-width:2px,color:#000,font-size:20px
style C1 fill:#f9dce6,stroke:#333,stroke-width:2px,color:#000,font-size:20px
style C1a fill:#f9e9dc,stroke:#333,stroke-width:2px,color:#000,font-size:20px
style D1 fill:#dcf9e9,stroke:#333,stroke-width:2px,color:#000,font-size:20px
style D1a fill:#dceff9,stroke:#333,stroke-width:2px,color:#000,font-size:20px
style E1 fill:#f9e9dc,stroke:#333,stroke-width:2px,color:#000,font-size:20px

```
### Customized Model (~92.7M)
```mermaid
graph LR
    A2["T5ForConditionalGeneration<br>~92.7M<br>parameters"] --> 
    B2["Shared Embedding<br>37780Ã—512<br>â‰ˆ 19.3M"] --> 
    C2["Encoder 7 layers + LoRA + Adapters<br>â‰ˆ 22.6M"] --> 
    C2a["Each Encoder Block<br>â‰ˆ 3.23M<br>- Self-Attention â‰ˆ 1.07M<br>- LoRA + Feed Forward + Adapter â‰ˆ 2.16M"] --> 
    D2["Decoder 7 layers + Adaptive Attention<br>â‰ˆ 31.5M"] --> 
    D2a["Each Decoder Block<br>â‰ˆ 4.5M<br>- Self-Attention â‰ˆ 1.07M<br>- Cross-Attention â‰ˆ 1.07M<br>- Feed Forward â‰ˆ 2.1M<br>- Adaptive Gate â‰ˆ 0.26M"] --> 
    E2["LM Head<br>512â†’37780<br>â‰ˆ 19.3M"]

   style A2 fill:#a3b2e6,stroke:#333,stroke-width:2px,color:#000,font-size:20px
    style B2 fill:#c3a3e6,stroke:#333,stroke-width:2px,color:#000,font-size:20px
    style C2 fill:#e6a3c0,stroke:#333,stroke-width:2px,color:#000,font-size:20px
    style C2a fill:#e6c4a3,stroke:#333,stroke-width:2px,color:#000,font-size:20px
    style D2 fill:#a3e6c4,stroke:#333,stroke-width:2px,color:#000,font-size:20px
    style D2a fill:#a3d4e6,stroke:#333,stroke-width:2px,color:#000,font-size:20px
    style E2 fill:#e6d3a3,stroke:#333,stroke-width:2px,color:#000,font-size:20px

```
| ğŸ”¹ Term        | Definition                                                                 |
|----------------|---------------------------------------------------------------------------|
| **LLM**        | AI model trained on massive text data to understand, generate, and translate human-like language. |
| **Transformer**| Neural architecture using self-attention to model sequences efficiently; backbone of modern LLMs. |
| **T5 Small**   | Lightweight Transformer-based Seq2Seq model for text tasks; pretrained on English, limited resources. |


### ğŸš€ Seq2Seq T5 Model â€“ Purposeful Challenges & Customization

1. **ğŸ“‚ Data Preparation**  
   - Generated **Train, Test, Val** separately from Grok, ChatGPT, Gemini.  
   - Three categories:  
     1ï¸âƒ£ Mixed Bangla-English in English  
     2ï¸âƒ£ Pure Bangla in Bangla words  
     3ï¸âƒ£ Mixed Bangla-English (English in English Letter and Bangla in Bangla Letter)  
   - **Limited dataset**, no external Bangla text included.  
   - **ğŸ¯ Target column**: English translation â€“ purpose: teach the new language to translate into English.

2. **âš™ï¸ Model Selection**  
   - **T5 Small** chosen for resource constraints.  
   - Model is **unaware of Bangla**.  

3. **ğŸ“ Vocabulary & Tokenization**  
   - Extract vocab only from **generated Train & Val data**.  
   - Combine **T5 English tokens + new Bangla tokens**.  
   - Use **SentencePiece** for uniform tokenization.  

4. **ğŸ”§ T5 Configuration Changes**  
   - Increase **dropout & attention_dropout** for stability.  
   - Add **extra encoder & decoder layers** for capacity.  
   - Freeze base layers to preserve English knowledge.  

5. **ğŸ§© Custom Layers / Blocks**  
   - **Adapter Layers**: lightweight residual blocks for parameter-efficient fine-tuning.  
   - **LoRA**: low-rank matrices on Q/K/V to inject Bangla patterns.  
   - **Adaptive Attention**: gating between decoder & encoder states.  

6. **âš¡ Optimizer & Scheduler**  
   - Use **Adafactor** with automatic **relative step LR** and **warmup**.  

7. **ğŸ“Š Performance Evaluation**  
   - **BLEU**: n-gram overlap (translation accuracy)  
   - **ROUGE-L**: longest common subsequence (fluency)  
   - **Perplexity / Log-Likelihood**: model confidence & prediction probability  
   - **Validation monitoring** with early stopping to prevent overfitting.  


# ğŸ› ï¸ Model Customization

| #   | Component     | Purpose / Role                                     | Importance                                                                                       |
| --- | ------------- | -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |
| 1ï¸âƒ£ | Embeddings    | Initial layer for input tokens                     | ğŸ›¡ï¸ Preserve pretrained embeddings, reduce memory, ensure stability on small dataset                             |
| 2ï¸âƒ£ | Frozen Layers | Retain pretrained knowledge in lower layers        | âš¡ Maintains generalization, prevents catastrophic forgetting, adapts efficiently to Bangla â†’ English translation |
| 3ï¸âƒ£ | Encoder       | Maps Bangla/Banglish input â†’ latent semantic space | âœ… Preserve pretrained knowledge, adapt only higher layers to Bangla                                              |
| 4ï¸âƒ£ | Decoder       | Converts latent embeddings â†’ English output tokens | ğŸ¯ Learn mapping from Bangla latent space â†’ fluent English sequences                                             |
| 5ï¸âƒ£ | LM Head       | Generates final logits for output tokens           | âœï¸ Required to produce correct English words corresponding to latent representation                              |

### Model config (after your edits)
| Setting                      | Value                              | Purpose (plain English)                                  |
|-----------------------------|------------------------------------|-----------------------------------------------------------|
| Base model                  | t5-small                           | Start from strong pretrained weights                      |
| Encoder layers              | 7 (6 + 1 extra)                    | Extra capacity to learn Bangla nuances                    |
| Decoder layers              | 7 (6 + 1 extra)                    | Extra capacity for fluent, faithful English generation    |
| dropout_rate                | 0.3 (â†‘ from 0.1)                   | Reduce overfitting; make representations more robust      |
| attention_dropout_rate      | 0.3 (â†‘ from 0.1)                   | Regularize attention; prevent head over-reliance          |
| layerdrop                   | 0.2                                | Randomly drop layers during training â†’ better generalization |
| Relative attention bias     | First block only (unchanged)       | Stable positional inductive bias                          |
| Extra blocksâ€™ rel. bias     | `has_relative_attention_bias=False`| Keep bias placement consistent with T5 design             |

### Training plan (what to freeze vs train)
| Part                       | Trainable? | Purpose (why this choice)                                       |
|---------------------------|------------|------------------------------------------------------------------|
| `model.shared` embedding  | âŒ Frozen   | Keep core token meanings stable across Bangla/English            |
| Encoder layers 0â€“2        | âŒ Frozen   | Preserve general syntax/semantics from pretraining               |
| Encoder layers 3â€“7        | âœ… Train    | Adapt higher layers to Bangla grammar & context                  |
| Decoder layers 0â€“7       | âœ… Train    | Produce fluent, accurate English aligned to Bangla input         |
| LM head (output layer)    | âœ… Train    | Map decoder states â†’ target vocabulary effectively                |

> Notes: layer indices are 0-based. With your added blocks, there are **7 encoder** and **7 decoder** layers total.

## ğŸš€ Updating the Encoder Layers

| #  | Module / Concept               | Key Idea / Iconography                                                   | Chosen Value & Reasoning |
|----|--------------------------------|-------------------------------------------------------------------------|-------------------------|
| 1ï¸âƒ£ | Adapter                        | ğŸ”§ Bottleneck FFN layer; down â†’ ReLU â†’ up; residual connection; lightweight extra brain; preserves pretrained knowledge | `bottleneck = 64` â†’ reduces dimensionality for efficiency, preserves key FFN features, adds residual for stability |
| 2ï¸âƒ£ | LoRA: Preserve Knowledge       | ğŸ§  Base model frozen; ğŸª„ LoRA trains small matrices; ğŸ›¡ï¸ avoids forgetting | `r = 8` â†’ low-rank; balances trainable parameters and learning capacity |
| 3ï¸âƒ£ | LoRA: Efficient Adaptation     | âš¡ Small dataset; ğŸ“‰ less compute; ğŸ“ captures new vocab & grammar       | Uses low-rank adapters â†’ efficient adaptation without full fine-tuning |
| 4ï¸âƒ£ | LoRA: Task & Language Specific | ğŸ¯ Target key layers (Q/K/V); ğŸ’¡ efficient adaptation                   | Focus on attention matrices â†’ most impact for cross-lingual transfer |
| 5ï¸âƒ£ | LoRA: Swap / Combine Languages | ğŸ”„ Multiple adapters; ğŸŒ load per language; ğŸ—ï¸ base model unchanged     | Separate adapters per language â†’ modular and flexible multilingual support |
| ğŸ’¡ | LoRA Analogy                   | ğŸ“˜ Base = polyglot student; LoRA = Bangla phrasebook, lightweight & safe| Analogy clarifies purpose and efficiency |
| 6ï¸âƒ£ | LoRA Integration               | ğŸ—ï¸ LoRA added to q/k/v in all encoder blocks; Adapter added from block 2 onwards; efficiently adapts model without full fine-tuning | Combines Adapter + LoRA â†’ minimal trainable parameters, maximal adaptation |

## ğŸš€ Add Adaptive Attention in Decoder Sections

| #  | Module / Concept           | Key Idea / Iconography                                                   | Chosen Value & Reasoning |
|----|----------------------------|-------------------------------------------------------------------------|-------------------------|
| 1ï¸âƒ£ | Adaptive Attention + Gating | ğŸ¯ Gate network adaptively combines encoder & decoder states; âš¡ context scaled by learnable parameter; ğŸ”‘ improves cross-attention focus | `hidden_size = 512` â†’ matches T5-small decoder; `scale = ones(hidden_size)` â†’ learnable scaling preserves magnitude; gate = linear(hidden_size, hidden_size) for per-dim adaptive weighting |
| 2ï¸âƒ£ | Integration into Decoder   | ğŸ—ï¸ Added after FFN / layer[1] in each decoder block; replaces static cross-attention output with gated adaptive output | Ensures decoder effectively balances source context & own hidden states; improves translation accuracy without modifying base attention |

# ğŸ† Model Training and Evaluation

| #  | Name / Type                     | Short Code Summary / Purpose                                                                 |
|----|---------------------------------|---------------------------------------------------------------------------------------------|
| 1ï¸âƒ£ | `create_optimizer_and_scheduler` | Function: Creates Adafactor optimizer with built-in dynamic LR schedule; no external scheduler needed. |
| 2ï¸âƒ£ | `Seq2SeqTrainer`                 | Class: Implements custom seq2seq training loop, validation, early stopping, and model saving. |
| 3ï¸âƒ£ | `__init__`                       | Method: Initializes trainer; sets model, tokenizer, dataloaders, device, optimizer, scaler, epochs, patience, and tracking variables. |
| 4ï¸âƒ£ | `run_train_epoch`                | Method: Performs one epoch of training; uses mixed precision (`torch.amp`), gradient clipping, and tracks training loss. |
| 5ï¸âƒ£ | `run_val_epoch`                  | Method: Performs one epoch of validation; computes average loss; tracks total validation time. |
| 6ï¸âƒ£ | `save_model`                     | Method: Saves model and tokenizer to disk; prints path of saved model. |
| 7ï¸âƒ£ | `fit`                            | Method: Main training loop; runs train + validation per epoch, applies early stopping, tracks total time. |
| 8ï¸âƒ£ | `plot_losses`                    | Method: Plots training and validation loss curves for visual inspection of convergence. |
| 9ï¸âƒ£ | `optimizer = Adafactor(...)`     | Adafactor optimizer: Supports relative step learning rate, warmup, and scaling; used for large-scale seq2seq training. |
| ğŸ”Ÿ | `self.scaler = GradScaler(...)`  | Mixed precision: Scales gradients to prevent underflow in float16 training; only enabled on GPU. |
| 11ï¸âƒ£ | `torch.nn.utils.clip_grad_norm_` | Clips gradients to max norm (1.0) to stabilize training and prevent exploding gradients. |
| 12ï¸âƒ£ | `tqdm(self.train_dataloader)`   | Provides progress bar for each training batch with live loss updates. |
| 13ï¸âƒ£ | `total_val_time`                | Tracks total cumulative validation time across all epochs; used for monitoring efficiency. |

# ğŸ¯ Summary

The original T5 model had no Bangla knowledge. We **added 5,680 new tokens** for mixed Bangla-English input, enabling the model to **start learning Bangla** while retaining English.

Training and validation datasets were prepared separately. **Training loss dropped steadily**, showing effective learning, while **validation loss plateaued** (~2.39â€“2.46), triggering **early stopping** after 8 patience epochs.

**Key Observations:**  
- ğŸ”¹ **Training Loss:** dropped from **0.8820 â†’ 0.7875** over the last few epochs.  
- ğŸ”¹ **Validation Loss:** remained **2.39â€“2.46**, indicating stable but limited generalization.  
- ğŸ”¹ **Early Stopping:** triggered after **8 epochs** without improvement, preventing overfitting.

**Justification:**  
- âœ… Training loss drop confirms the model is **learning Bangla tokens**.  
- âš ï¸ Validation plateau is expected due to **novel language and dataset size**.  
- ğŸ›¡ï¸ Early stopping ensures the model does not overfit. Overall, the model **successfully adapts T5 to Bangla** with custom tokens and mixed-language training.

# ğŸ“Š Performance Evaluation

| #  | Name / Type           | Short Code Summary / Purpose                                                                 |
|----|----------------------|---------------------------------------------------------------------------------------------|
| 1ï¸âƒ£ | `evaluate_model`      | Function: Evaluates a trained model on BLEU, ROUGE-1, ROUGE-L, Perplexity, and Log-Likelihood metrics. |
| 2ï¸âƒ£ | `model.eval()`        | Puts model in evaluation mode; disables dropout and gradient updates.                      |
| 3ï¸âƒ£ | `bleu_metric = load("sacrebleu")` | Initializes BLEU metric from `datasets` library for translation quality evaluation.       |
| 4ï¸âƒ£ | `rouge_metric = load("rouge")`   | Initializes ROUGE metric for summarization/sequence comparison.                           |
| 5ï¸âƒ£ | Tokenization          | Converts `text_input` and `english_targets` into tensors with padding/truncation for batching. |
| 6ï¸âƒ£ | Prediction Generation | Uses `model.generate` with `num_beams=4`, `max_length=128` for beam search decoding.       |
| 7ï¸âƒ£ | Decoding Predictions  | Converts generated token IDs back to text using `tokenizer.batch_decode`.                  |
| 8ï¸âƒ£ | Perplexity / NLL      | Computes average negative log-likelihood and perplexity per token for model confidence.    |
| 9ï¸âƒ£ | Metric Computation    | Computes BLEU score, ROUGE scores, log-likelihood, and perplexity from predictions & references. |
| ğŸ”Ÿ | Batching              | Processes data in batches (size=8) for memory efficiency on GPU/CPU.                       |
| 1ï¸âƒ£ | Return Dictionary    | Returns a dictionary containing: `log_likelihood`, `perplexity`, `bleu`, and `rouge` scores. |

| Metric                  | Value      | Purpose / Why We Use It                                | Interpretation / Justification                                      |
|-------------------------|-----------|-------------------------------------------------------|---------------------------------------------------------------------|
| Log-Likelihood (NLL) ğŸ”¥  | -5.2099   | Measures modelâ€™s confidence in predicting tokens      | Negative value expected; shows model predicts tokens reasonably, but still uncertain on new Bangla patterns |
| Perplexity ğŸ¯            | 183.08    | Measures prediction uncertainty; lower = better      | High perplexity indicates Bangla is still challenging for T5, due to limited exposure and dataset size |
| BLEU ğŸ“Š                  | 1.76      | Measures n-gram overlap with reference translation   | Very low BLEU reflects difficulty in exact Bangla-English translation; expected for first adaptation |
| ROUGE-1 ğŸ“               | 0.1319    | Measures unigram overlap / content coverage          | Low overlap shows generated text partially matches references; model starting to learn Bangla |
| ROUGE-2 ğŸ“               | 0.0208    | Measures bigram overlap / phrase similarity          | Very low score indicates limited phrase-level accuracy; expected with small dataset |
| ROUGE-L / ROUGE-Lsum ğŸ“  | 0.1178 / 0.1183 | Measures longest common subsequence / overall structure | Confirms partial structural learning; model captures some word order but still limited |

# Resulted Output 

![alt text](image.png)

### Justification â€“ ROUGE-L & Log-Likelihood

| #   | ROUGE-L Score | Log-Likelihood | Justification |
|-----|---------------|----------------|---------------|
| 0   | 0.1856        | -65.8346       | Moderate overlap with reference; model reasonably predicts festive greeting. |
| 1   | 0.0606        | -120.0762      | Low overlap; birthday sentence structure differs from model output, higher NLL indicates uncertainty. |
| 10  | 0.2535        | -147.5672      | Higher ROUGE-L shows partial correct sequence; negative log-likelihood high due to token errors. |
| 57  | 0.0800        | -115.7060      | Small overlap; model struggles with question structure; NLL indicates prediction uncertainty. |
| 58  | 0.0645        | -173.3233      | Very low overlap; model fails to capture future tense; high NLL reflects poor token probability. |
| 60  | 0.0294        | -119.8803      | Minimal overlap; conversational tone not well captured, moderate NLL. |
| 112 | 0.0968        | -239.6151      | Low overlap for long mixed Bangla-English sentence; very high NLL due to complexity. |
| 114 | 0.0267        | -166.4844      | Almost no overlap; document-related context not learned; high NLL. |
| 118 | 0.1034        | -93.9555       | Slightly better overlap; model partially captures evening query, moderate NLL. |
| 11  | 0.1231        | -80.9562       | Fair ROUGE-L; rickshaw request partially understood, lower NLL reflects simpler tokens. |




## Authors

- [@LinkedIn Khan MD Shibli Nomani](https://www.linkedin.com/in/khan-md-shibli-nomani-45445612b/)